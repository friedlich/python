文本特征提取
词袋（Bag of Words）表征

文本分析是机器学习算法的主要应用领域。但是，文本分析的原始数据无法直接丢给算法，这些原始数据是一组符号，因为大多数算法期望的输入是固定长度的数值特征向量而不是不同长度的文本文件。为了解决这个问题，scikit-learn提供了一些实用工具可以用最常见的方式从文本内容中抽取数值特征，比如说：

    标记（tokenizing）文本以及为每一个可能的标记（token）分配的一个整型ID ，例如用白空格和标点符号作为标记的分割符（中文的话涉及到分词的问题）
    计数（counting）标记在每个文本中的出现频率
    正态化(nomalizating) 降低在大多数样本/文档中都出现的标记的权重

在这个方案中，特征和样本的定义如下：

将每个标记出现的频率(无论是否正态化)作为特征。

给定文件中所有标记的出现频率所构成的向量作为多元样本。

因此，语料文件可以用一个词文档矩阵代表，每行是一个文档，每列是一个标记（即词）。

将文档文件转化为数值特征的一般过程被称为向量化。这个特殊的策略（标记，计数和正态化）被称为词袋或者Bag of n-grams表征。用词频描述文档，但是完全忽略词在文档中出现的相对位置信息。

稀疏性

大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。

例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。

为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征
